---
title: "1st meeting preparation"
author: "Julien"
date: "6 Jan 2016"
output:
  html_document:
    toc: yes
---

My Ideas before the meeting
========

Our goal here is not to complain, but to build the best peer review system possible.

Prospositions:

1. Independent of manuscript depositions 
    - Pre- and Post- publication review possible 
    - Focus on one problem: peer review
    - No need of new manuscript in the system to start (all DOI objects are peer-reviewable directly)
    
2. We deal first with the ideal system, we will determine the way to go there later
    - We do not care about scientist's habit
    - We do not think about business models
    - We do not look at what exists


    
3. Goal of peer review    
    - constructive to improve text/interpretation quality
    - check for appropriate methodology
    - check factual accuracy
    - correct spelling/grammar
    - set novelty and importance ?
    - detect fraud?
    

Participants
============

these people show interest, we were about 12 people in the meeting.
```{r, echo=F}
library(pander)
b=read.csv("participants.csv", sep=";")
b[is.na(b)]<-""
pander (b)

```



Notes
=====

Long and lively discussion and a nice and working atmosphere. Most of us were passionate about the topic and came for building something.

Summary
-------
After an introduction by Dr. Colomb about the goal of the event (see above), we went on fruitful, unmoderated discussion about the "ideal peer review system". One of the idea coming from that discussion is that we may think about a system to review grants before they are sent to the grant agencies: authors and editors would have different (more honorable) motivation in this perspective. Once we draw the ideal system for this review system, we can go on and see if it is applicable to replace the present peer-review system (or if it has to be tweaked somehow). Then we looked into more details about the motivation of the three stakeholders: authors, reviewers and editors (one single person may have the 3 roles in different situations).

2 parts of peer-review
----------------
After some discussion, we separated the peer review process into 2 entities: One more objective part:
- Field specific requirement achieved?
- Reproducibility of the data analysis
- Reproducibility of the data
- Is the idea flow and the language used comprehensible

and one more subjective:
- Result importance and novelty
- Novely of the method used,...
- What is the paper audience, can the text be better to 

With the idea that some of these steps may be automatized using text mining as a data source and producing a checkboard for the paper (check for plagiarism, check for field specific requirements, check for novelty maybe too).

Reviewer quality
----------------

There is little training for reviewers, and no consensus about how a review should look like. Checklist appear to be useful (both to be faster and more accurate in the review). One needs both critics and positive comments.

One could differentiate between comments and reviews, depending on the quality of the reviewer. This may be also more complexe than this binary scale.

The problem with comments from the crowd (reserchgate, stack exchange) is the noise produced by bad quality reviews. crowd giving reviewers credit is also not working well, since answer to easy questions are giving more credit than answers to difficult ones.

What should be reviewed
----------------

We had interesting discussion on reviewing grants as well as experiment methodology before it is used, additionally scientific manuscripts should be reviewed pre- and post-publication. Indeed, the "publication" step (quality stamp through publication) should disappear for a more flexible paper quality ranking (some possible stamps: good for reviews, peer reviewed for methodolgy, peer reviewed for interpretation, replicated,..)

OPEN?
-----

We did not discuss much about questions like: does it need to be done blind, double blind, in the open? Some thinks the system should not be open:

    The real time process of the review should not be open or even open to comments. There should be the room of privacy and security between editors-authors-reviewers while under review.

Stakeholders motivation
=====================

We focus this to the situation of a grant send for pre-review, before the grant is actually sent to the grant agencies, in order to get the largest pannel of positive motivation.

Rene Bernard does not agree with this proposition, arguing that "none of us really has experience as grant reviewers and very little as grant applicant. We should focus of the paper review first and maybe later see which elements we came up with apply to grants."

Authors expectations
-------

    Get a rapid feedback to get a better, quality approved study
    
 system:
1. Quick
2. Short feedback (no "noise" feedback)
3. Review from the targeted audience (here putative grant reviewers)
4. Direct communication with reviewers
Review:
5. ameliorate methodology
6. ameliorate text 
7. get a greater audience (=more citation)
8. Stamp of approval

**5 and 6 mostly implicate the riviewer must be comptent.**

Reviewers expectations
------

    Invest minimal work for a maximal credit
    
1. GET CREDIT
2. Review only review-ready manuscript
3. Helping tools to be faster (checklist, automation)
4. Easy to join

Editors expectations
-----

    Will the study be read/cited?
    
1. Quality check
2. Get an ideas on study audience 
3. Get an idea on study importance

Additional Comments and feedback
========
Rene Bernard made some comments integrated in the notes, in addition, we would need inputs from professional editors for the next meeting.
